{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rakuten France Multimodal Product Data Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this challenge is to perform large-scale product type code multimodal classification using both text and image data. The aim is to predict the type code for each product based on the catalog of Rakuten France."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing product listings through title and image is a crucial task for any e-commerce marketplace, as it has various applications such as personalized search, recommendations, and query understanding. Manual and rule-based approaches to categorization are not scalable since there are numerous classes of commercial products. Multimodal approaches are a useful technique for e-commerce companies as they face difficulty in categorizing products based on images and labels from merchants, especially when dealing with both new and used products from professional and non-professional merchants, as is the case with Rakuten. However, the lack of real data from actual commercial catalogs has limited progress in this area of research. The challenge presents several interesting research aspects due to the noisy nature of product labels and images, the large size of modern e-commerce catalogs, and the typical distribution of unbalanced data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this data challenge is large-scale multimodal (text and image) product data classification into product type codes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide an example, consider a product in the Rakuten France catalog with a French name or title \"Klarstein Présentoir 2 Montres Optique Fibre\" and associated image, and possibly an additional description. This product is classified under the 1500 product type code. There are other products with varying titles, images, and descriptions that fall under the same product type code. This challenge aims to develop a classifier that can accurately categorize products into their corresponding product type code based on information such as the example given above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric used in this challenge to rank the participants is the *weighted-F1 score*.\n",
    "\n",
    "Scikit-Learn package has an F1 score implementation and can be used for this challenge with its average parameter set to \"weighted\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, Rakuten France is releasing approximatively 99K product listings in CSV format, including the train (84,916) and test set (13,812). The dataset consists of product designations, product descriptions, product images and their corresponding product type code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are divided under two criteria, forming four distinct sets: training or test, input or output.\n",
    "\n",
    "* X_train.csv: training input file\n",
    "* Y_train.csv: training output file\n",
    "* X_test.csv: test input file\n",
    "\n",
    "Additionally images.zip file is supplied containing all the images. Uncompressing this file will provide a folder named images with two subfolders named image_training and image_test, containing training and test images respectively.\n",
    "\n",
    "The first line of the input files contains the header, and the columns are separated by comma (\",\"). The columns are:\n",
    "\n",
    "* An integer ID for the product. This ID is used to associate the product with its corresponding product type code.\n",
    "* **designation** - The product title, a short text summarizing the product.\n",
    "* **description** - A more detailed text describing the product. Not all the merchants use this field, so to retain originality of the data, **the description field can contain NaN value for many products**.\n",
    "* **productid** - An unique ID for the product.\n",
    "* **imageid** - An unique ID for the image associated with the product.\n",
    "\n",
    "The fields **imageid** and **productid** are used to retrieve the images from the respective image folder. For a particular product the image file name is image_imageid_product_productid.jpg."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of an input file:\n",
    "\n",
    ",designation,description,productid,imageid\n",
    "0,Olivia: Personalisiertes Notizbuch 150 Seiten Punktraster Ca Din A5 Rosen-Design,,3804725264,1263597046\n",
    "1,Journal Des Arts (Le) NÃ Â° 133 Du 28/09/2001 - L'art Et Son Marche Salon D'art Asiatique A Paris - Jacques Barrere - Francois Perrier - La Reforme Des Ventes Aux Encheres Publiques - Le Sna Fete Ses Cent Ans.,,436067568,1008141237\n",
    "\n",
    "For the first product the corresponding image file name is image_1263597046_product_3804725264.jpg, and the same for the second product is image_1008141237_product_436067568.jpg. One can recall that all the images corresponding to the training products listed in X_train.csv can be found in image_training subfolder, and all the images corresponding to the test products listed in X_test.csv can be found in image_test subfolder.\n",
    "\n",
    "The training output file (Y_train.csv) contains the prdtypecode, the category for the classification task, for each integer id in the training input file (X_train.csv). Here also the first line of the file is the header and columns are separated by commas.\n",
    "\n",
    "Here is an example of the output file:\n",
    "\n",
    ",prdtypecode\n",
    "0,10\n",
    "1,2280\n",
    "\n",
    "For the test input file X_test.csv, participants need to provide a test output file in the same format as the training output file (associating each integer id with the predicted prdtypecode). The first line of this test output file should contain the header ,prdtypecode."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark algorithm uses two separate models for the images and the text. Participants can get an idea of the performances when these sources of informations are used separately. They are encouraged to use both these sources while designing a classifier, since they contain complementary information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the image data, a version of Residual Networks (ResNet) model (**reference**) is used. ResNet50 implementation from Keras is used as the base model. The details of the basic benchmark model can be found **in this notebook**. The model is a pre-trained ResNet50 with ImageNet dataset. 27 different layers from top are unfrozen, which include 8 Convolutional layers for the training. The final network contains 12,144,667 trainable and 23,643,035 non-trainable parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the text data a simplified CNN classifier used. Only the designation fields (product titles) are used in this benchmark model. The input size is the maximum possible designation length, 34 in this case. Shorter inputs are zero-padded. The architecture consists of an embedding layer and 6 convolutional, max-pooling blocks. The embeddings are trained with the entire architecture. Following is the model architecture:\n",
    "\n",
    "Layer (type)\t        Output Shape\t        Number of Params\tConnected to\n",
    "InputLayer\t            (None, 34)\t            0\t\n",
    "Embedding Layer\t        (None, 34, 300)\t        17320500\t        InputLayer\n",
    "Reshape\t                (None, 34, 300, 1)\t    0\t                Embedding Layer\n",
    "Conv2D Block 1\t        (None, 34, 1, 512)\t    154112\t            Reshape\n",
    "MaxPooling2D Block 1\t(None, 1, 1, 512)\t    0\t                Conv2D Block 1\n",
    "Conv2D Block 2\t        (None, 33, 1, 512)\t    307712\t            Reshape\n",
    "MaxPooling2D Block 2\t(None, 1, 1, 512)\t    0\t                Conv2D Block 2\n",
    "Conv2D Block 3\t        (None, 32, 1, 512)\t    461312\t            Reshape\n",
    "MaxPooling2D Block 3\t(None, 1, 1, 512)\t    0\t                Conv2D Block 2\n",
    "Conv2D Block 4\t        (None, 31, 1, 512)\t    614912\t            Reshape\n",
    "MaxPooling2D Block 4\t(None, 1, 1, 512)\t    0\t                Conv2D Block 2\n",
    "Conv2D Block 5\t        (None, 30, 1, 512)\t    768512\t            Reshape\n",
    "MaxPooling2D Block 5\t(None, 1, 1, 512)\t    0\t                Conv2D Block 2\n",
    "Conv2D Block 6\t        (None, 29, 1, 512)\t    922112\t            Reshape\n",
    "MaxPooling2D Block 6\t(None, 1, 1, 512)\t    0\t                Conv2D Block 2\n",
    "Concatenate\t            (None, 6, 1, 512)\t    0\t                All MaxPooling2D Blocks\n",
    "Flatten\t                (None, 3072)\t        0\t                Concatenate\n",
    "Dropout Layer\t        (None, 3072)\t        0\t                Flatten\n",
    "Dense Layer\t            (None, 27)\t            8297\t            Dropout Layer\n",
    "\n",
    "This architecture contains total 20,632,143 trainable parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Layer (type)-----------------Output Shape-----------Number of Params------Connected to\n",
    "* InputLaye-------------------(None, 34)--------------0\t\n",
    "* Embedding Layer-----------(None, 34, 300)---------17320500----------------InputLayer\n",
    "* Reshape---------------------(None, 34, 300, 1)------0--------------------------Embedding Layer\n",
    "* Conv2D Block 1-------------(None, 34, 1, 512)------154112-------------------Reshape\n",
    "* MaxPooling2D Block 1------(None, 1, 1, 512)-------0--------------------------Conv2D Block 1\n",
    "* Conv2D Block 2-------------(None, 33, 1, 512)------307712-------------------Reshape\n",
    "* MaxPooling2D Block 2------(None, 1, 1, 512)-------0--------------------------Conv2D Block 2\n",
    "* Conv2D Block 3-------------(None, 32, 1, 512)------461312-------------------Reshape\n",
    "* MaxPooling2D Block 3------(None, 1, 1, 512)-------0--------------------------Conv2D Block 2\n",
    "* Conv2D Block 4-------------(None, 31, 1, 512)------614912-------------------Reshape\n",
    "* MaxPooling2D Block 4------(None, 1, 1, 512)-------0--------------------------Conv2D Block 2\n",
    "* Conv2D Block 5-------------(None, 30, 1, 512)------768512-------------------Reshape\n",
    "* MaxPooling2D Block 5------(None, 1, 1, 512)-------0--------------------------Conv2D Block 2\n",
    "* Conv2D Block 6-------------(None, 29, 1, 512)------922112-------------------Reshape\n",
    "* MaxPooling2D Block 6------(None, 1, 1, 512)-------0--------------------------Conv2D Block 2\n",
    "* Concatenate-----------------(None, 6, 1, 512)-------0--------------------------All MaxPooling2D Blocks\n",
    "* Flatten-----------------------(None, 3072)-----------0--------------------------Concatenate\n",
    "* Dropout Layer---------------(None, 3072)-----------0--------------------------Flatten\n",
    "* Dense Layer-----------------(None, 27)--------------8297----------------------Dropout Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the weighted-F1 score obtained using the benchmark models described above:\n",
    "\n",
    "Text: 0.8113\n",
    "\n",
    "Images: 0.5534\n",
    "\n",
    "As the benchmarking model using text is better performing, the Y benchmark file contains the output of the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba554774fb2605650abed9b436f1953a32fd8980d4057a3e0602fd4cb3aacd7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
