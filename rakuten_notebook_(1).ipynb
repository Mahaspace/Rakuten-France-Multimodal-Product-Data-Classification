{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpJlwtMp4nny"
      },
      "source": [
        "# Rakuten France Multimodal Product Data Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoiDbprV5G92"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "<p><div class=\"lev1 toc-item\"><a href=\"#RAMP-on-Rakuten France Multimodal product data Classification\" data-toc-modified-id=\"RAMP-on-Rakuten France Multimodal product data Classification\" ></div><div class=\"lev2 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev2 toc-item\">\n",
        "<a href=\"#Problem description\" data-toc-modified-id=\"Problem description-1.1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Problem description</a></div><div class=\"lev2 toc-item\"><a href=\"#Getting-started-with-the-RAMP-starting-kit\" data-toc-modified-id=\"Getting-started-with-the-RAMP-starting-kit-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Getting started with the RAMP starting kit</a></div><div class=\"lev3 toc-item\"><a href=\"#Software-prerequisites\" data-toc-modified-id=\"Software-prerequisites-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Software prerequisites</a></div><div class=\"lev3 toc-item\"><a href=\"#Getting-the-data\" data-toc-modified-id=\"Getting-the-data-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Getting the data</a></div><div class=\"lev2 toc-item\"><a href=\"#The-data\" data-toc-modified-id=\"The-data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>The data</a></div><div class=\"lev3 toc-item\"></div><div class=\"lev3 toc-item\"><a href=\"#Multi-Class Imbalanced Classification\" data-toc-modified-id=\"Multi-Class Imbalanced Classification-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Multi-Class Imbalanced Classification</a></div><div class=\"lev3 toc-item\"></div><div class=\"lev2 toc-item\"><a href=\"#Workflow\" data-toc-modified-id=\"Workflow-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Workflow</a></div><div class=\"lev3 toc-item\"><a href=\"#The-model-to-submit\" data-toc-modified-id=\"The-model-to-submit-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>The model to submit</a></div><div class=\"lev3 toc-item\"><a href=\"#Metric\" data-toc-modified-id=\"Metric-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Metric</a></div><div class=\"lev3 toc-item\"></div><div class=\"lev2 toc-item\"><a href=\"#Submitting-to-the-online-challenge:-ramp.studio\" data-toc-modified-id=\"Submitting-to-the-online-challenge:-ramp.studio-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Submitting to the online challenge: <a href=\"http://ramp.studio\" target=\"_blank\">ramp.studio</a></a></div><div class=\"lev2 toc-item\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYN1UmuY4nn5"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z90MMHlM4nn6"
      },
      "source": [
        "The objective of this challenge is to perform large-scale product type code multimodal classification using both text and image data. The aim is to predict the type code for each product based on the catalog of Rakuten France."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHO5_XF44nn7"
      },
      "source": [
        "Categorizing product listings through title and image is a crucial task for any e-commerce marketplace, as it has various applications such as personalized search, recommendations, and query understanding. Manual and rule-based approaches to categorization are not scalable since there are numerous classes of commercial products. Multimodal approaches are a useful technique for e-commerce companies as they face difficulty in categorizing products based on images and labels from merchants, especially when dealing with both new and used products from professional and non-professional merchants, as is the case with Rakuten. However, the lack of real data from actual commercial catalogs has limited progress in this area of research. The challenge presents several interesting research aspects due to the noisy nature of product labels and images, the large size of modern e-commerce catalogs, and the typical distribution of unbalanced data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHfV48GF4nn8"
      },
      "source": [
        "## Problem description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n74XZZNQ4nn8"
      },
      "source": [
        "The goal of this data challenge is large-scale multimodal (text and image) product data classification into product type codes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEW19Gkz4nn9"
      },
      "source": [
        "To provide an example, consider a product in the Rakuten France catalog with a French name or title \"Klarstein Présentoir 2 Montres Optique Fibre\" and associated image, and possibly an additional description. This product is classified under the 1500 product type code. There are other products with varying titles, images, and descriptions that fall under the same product type code. This challenge aims to develop a classifier that can accurately categorize products into their corresponding product type code based on information such as the example given above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeJWN2gB7ocp"
      },
      "source": [
        "## Getting started with the RAMP starting kit\n",
        "\n",
        "### Software prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OamTpYEY7xYJ"
      },
      "source": [
        "This starting kit requires the following dependencies:\n",
        "\n",
        "* `numpy`\n",
        "* `pandas`\n",
        "* `pyarrow`\n",
        "* `scikit-learn`\n",
        "* `matplolib`\n",
        "* `jupyter`\n",
        "* `imbalanced-learn`\n",
        "\n",
        "We recommend to install those using `conda` (using the [`Anaconda`](https://www.anaconda.com/distribution/) distribution)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iHUx6_371To"
      },
      "source": [
        "In addition, `ramp-workflow` is needed. This can be installed from the master branch on GitHub:\n",
        "\n",
        "    python -m pip install https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51oHfo8i4noS",
        "outputId": "292aa595-9249-4391-a75d-5f5f0e1a627a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading the data files. This could take a couple of minutes...\n",
            "Downloading the images. this will take a while...\n"
          ]
        }
      ],
      "source": [
        "!python3 download_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8FNIJOr8Ssk"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2taKxmID8c93"
      },
      "source": [
        "## The data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43DD86AF4noC"
      },
      "source": [
        "For this challenge, Rakuten France is releasing approximatively 99K product listings in CSV format, including the train (67,932) and test set (16,984). The dataset consists of product designations, product descriptions, product images and their corresponding product type code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv28ZntV4noT"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('./data/public/train.csv')\n",
        "X_train = train_data.iloc[:,:-1]\n",
        "y_train = train_data.iloc[:,-1]\n",
        "test_data = pd.read_csv('./data/public/test.csv')\n",
        "X_test = test_data.iloc[:,:-1]\n",
        "y_test = test_data.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "JXYV9BVj4noU",
        "outputId": "9c78b5f3-2680-43a1-aa80-3afc50b55929"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>designation</th>\n",
              "      <th>description</th>\n",
              "      <th>productid</th>\n",
              "      <th>imageid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31823</td>\n",
              "      <td>Jemini Accroche Tétine Renard</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1848487330</td>\n",
              "      <td>1105385406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>77312</td>\n",
              "      <td>Jeu De Chaise Longue 3 Pcs Textilène Noir | Noir</td>\n",
              "      <td>Cet ensemble de deux chaises longues de haute ...</td>\n",
              "      <td>3855780079</td>\n",
              "      <td>1253262411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40705</td>\n",
              "      <td>Piscine Easy Set Intex 28110 2.44m x 0.76m</td>\n",
              "      <td>NaN</td>\n",
              "      <td>285447839</td>\n",
              "      <td>982002901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16531</td>\n",
              "      <td>Carte De Catch Evolution 2010 Ecw Abraham Wash...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>98368419</td>\n",
              "      <td>860440892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69033</td>\n",
              "      <td>Piscine d'Animaux Pataugeoire Pliable PVC Port...</td>\n",
              "      <td>Piscine d'Animaux Pataugeoire Pliable PVC Port...</td>\n",
              "      <td>3244916689</td>\n",
              "      <td>1201885812</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                        designation  \\\n",
              "0       31823                      Jemini Accroche Tétine Renard   \n",
              "1       77312   Jeu De Chaise Longue 3 Pcs Textilène Noir | Noir   \n",
              "2       40705         Piscine Easy Set Intex 28110 2.44m x 0.76m   \n",
              "3       16531  Carte De Catch Evolution 2010 Ecw Abraham Wash...   \n",
              "4       69033  Piscine d'Animaux Pataugeoire Pliable PVC Port...   \n",
              "\n",
              "                                         description   productid     imageid  \n",
              "0                                                NaN  1848487330  1105385406  \n",
              "1  Cet ensemble de deux chaises longues de haute ...  3855780079  1253262411  \n",
              "2                                                NaN   285447839   982002901  \n",
              "3                                                NaN    98368419   860440892  \n",
              "4  Piscine d'Animaux Pataugeoire Pliable PVC Port...  3244916689  1201885812  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74IGtHnb4noC"
      },
      "source": [
        "\n",
        "\n",
        "The data consist of 5 primary input variables:\n",
        "\n",
        "* An integer ID for the product. This ID is used to associate the product with its corresponding product type code.\n",
        "* **designation** - The product title, a short text summarizing the product.\n",
        "* **description** - A more detailed text describing the product. Not all the merchants use this field, so to retain originality of the data, **the description field can contain NaN value for many products**.\n",
        "* **productid** - An unique ID for the product.\n",
        "* **imageid** - An unique ID for the image associated with the product.\n",
        "\n",
        "\n",
        "Additionally images.zip file is supplied containing all the images. Uncompressing this file will provide a folder named images with two subfolders named image_training and image_test, containing training and test images respectively.\n",
        "\n",
        "The fields **imageid** and **productid** are used to retrieve the images from the respective image folder. For a particular product the image file name is image_imageid_product_productid.jpg.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzgIUhs54noV",
        "outputId": "20c2d395-a96c-4655-a65c-ae06d1ce274a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 67932 entries, 0 to 67931\n",
            "Data columns (total 5 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Unnamed: 0   67932 non-null  int64 \n",
            " 1   designation  67932 non-null  object\n",
            " 2   description  44013 non-null  object\n",
            " 3   productid    67932 non-null  int64 \n",
            " 4   imageid      67932 non-null  int64 \n",
            "dtypes: int64(3), object(2)\n",
            "memory usage: 2.6+ MB\n"
          ]
        }
      ],
      "source": [
        "X_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset is composed of integer and text columns. \n",
        "We can notice the fact that there are several NaN values in the description columns. This should be taken into account in your analysis."
      ],
      "metadata": {
        "id": "Ob61i-9jclxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Class Imbalanced Classification"
      ],
      "metadata": {
        "id": "FJZxYmjOe6nq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeJdVsw44noV",
        "outputId": "87805730-0396-41c6-9db2-3f23d1eeb303"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    1320\n",
              "1    2582\n",
              "2    2583\n",
              "3    1160\n",
              "4    2583\n",
              "Name: prdtypecode, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMcjtuKa4noV",
        "outputId": "18e0b56c-414c-490e-f6a2-aa1147a1282e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1320, 2582, 2583, 1160, 1560, 2585, 1920, 2280, 2705, 1300, 2060,\n",
              "       2403, 2522,   40, 1302, 1280, 1140,   50, 2462, 2220, 1180, 1301,\n",
              "       2905, 1940, 1281,   60,   10])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training output dataset contains the prdtypecode, the category for the classification task, for each integer id in the training input dataset.\n",
        "We can see that there is 27 different classes.\n"
      ],
      "metadata": {
        "id": "NWIlUSTpdZyQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "329kJfaF4noX",
        "outputId": "a7d73473-d2fc-436f-9c95-1c856db6e839"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:ylabel='Frequency'>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "y_train.value_counts().sort_index().plot.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class labels are unbalanced. One label contains more than 8000 elements in the training dataset and another contains about 4000. The majority of the class labels contain between 1000 and 3000 elements."
      ],
      "metadata": {
        "id": "TAjmeqNJfUa9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PES-FCsC4noX",
        "outputId": "3a130daa-335c-42b6-e752-0bc3b91e89c5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count NaN</th>\n",
              "      <th>Total</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prdtypecode</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2222</td>\n",
              "      <td>2492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1329</td>\n",
              "      <td>2029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>389</td>\n",
              "      <td>1356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>105</td>\n",
              "      <td>668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1140</th>\n",
              "      <td>1363</td>\n",
              "      <td>2121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1160</th>\n",
              "      <td>2871</td>\n",
              "      <td>3156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1180</th>\n",
              "      <td>509</td>\n",
              "      <td>636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>849</td>\n",
              "      <td>3853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1281</th>\n",
              "      <td>401</td>\n",
              "      <td>1675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1300</th>\n",
              "      <td>948</td>\n",
              "      <td>4009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1301</th>\n",
              "      <td>82</td>\n",
              "      <td>640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302</th>\n",
              "      <td>231</td>\n",
              "      <td>2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1320</th>\n",
              "      <td>899</td>\n",
              "      <td>2586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1560</th>\n",
              "      <td>151</td>\n",
              "      <td>4094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1920</th>\n",
              "      <td>152</td>\n",
              "      <td>3432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1940</th>\n",
              "      <td>37</td>\n",
              "      <td>638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2060</th>\n",
              "      <td>229</td>\n",
              "      <td>3980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>50</td>\n",
              "      <td>663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2280</th>\n",
              "      <td>3573</td>\n",
              "      <td>3818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2403</th>\n",
              "      <td>3739</td>\n",
              "      <td>3842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2462</th>\n",
              "      <td>1089</td>\n",
              "      <td>1134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2522</th>\n",
              "      <td>937</td>\n",
              "      <td>3960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2582</th>\n",
              "      <td>45</td>\n",
              "      <td>2071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2583</th>\n",
              "      <td>719</td>\n",
              "      <td>8145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2585</th>\n",
              "      <td>158</td>\n",
              "      <td>1979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2705</th>\n",
              "      <td>842</td>\n",
              "      <td>2230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2905</th>\n",
              "      <td>0</td>\n",
              "      <td>702</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             count NaN  Total\n",
              "prdtypecode                  \n",
              "10                2222   2492\n",
              "40                1329   2029\n",
              "50                 389   1356\n",
              "60                 105    668\n",
              "1140              1363   2121\n",
              "1160              2871   3156\n",
              "1180               509    636\n",
              "1280               849   3853\n",
              "1281               401   1675\n",
              "1300               948   4009\n",
              "1301                82    640\n",
              "1302               231   2023\n",
              "1320               899   2586\n",
              "1560               151   4094\n",
              "1920               152   3432\n",
              "1940                37    638\n",
              "2060               229   3980\n",
              "2220                50    663\n",
              "2280              3573   3818\n",
              "2403              3739   3842\n",
              "2462              1089   1134\n",
              "2522               937   3960\n",
              "2582                45   2071\n",
              "2583               719   8145\n",
              "2585               158   1979\n",
              "2705               842   2230\n",
              "2905                 0    702"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data\n",
        "df=train_data.description.isnull().groupby([train_data[\"prdtypecode\"]]).sum().astype(int).reset_index(name='count NaN')\n",
        "df.set_index(\"prdtypecode\",inplace=True)\n",
        "df[\"Total\"]=y_train.value_counts().sort_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen above, the description column contains many NaN values. But there are unevenly distributed among the different class labels. This could help predict the class label of the different products."
      ],
      "metadata": {
        "id": "mh_nuRrngXqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow"
      ],
      "metadata": {
        "id": "xtcWcAuHjOuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model to submit"
      ],
      "metadata": {
        "id": "qTMY2rU0iPaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The submission consists of ...."
      ],
      "metadata": {
        "id": "sT8KL3OtiTPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BjrgXhtA9go"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7nz8QiL4nn-"
      },
      "source": [
        "## Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv3OEWch4nn_"
      },
      "source": [
        "The metric used in this challenge to rank the participants is the *weighted-F1 score*.\n",
        "\n",
        "Scikit-Learn package has an F1 score implementation and can be used for this challenge with its average parameter set to \"weighted\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLt42mvn4noH"
      },
      "source": [
        "## Benchmark Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e1JwvzZ4noH"
      },
      "source": [
        "The benchmark algorithm uses two separate models for the images and the text. Participants can get an idea of the performances when these sources of informations are used separately. They are encouraged to use both these sources while designing a classifier, since they contain complementary information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CTBjnUU4noK"
      },
      "source": [
        "For the image data, a version of Residual Networks (ResNet) model (**reference**) is used. ResNet50 implementation from Keras is used as the base model. The details of the basic benchmark model can be found **in this notebook**. The model is a pre-trained ResNet50 with ImageNet dataset. 27 different layers from top are unfrozen, which include 8 Convolutional layers for the training. The final network contains 12,144,667 trainable and 23,643,035 non-trainable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VyUMx1I4noK"
      },
      "source": [
        "For the text data a simplified CNN classifier used. Only the designation fields (product titles) are used in this benchmark model. The input size is the maximum possible designation length, 34 in this case. Shorter inputs are zero-padded. The architecture consists of an embedding layer and 6 convolutional, max-pooling blocks. The embeddings are trained with the entire architecture. Following is the model architecture:\n",
        "\n",
        "Layer (type)\t        Output Shape\t        Number of Params\tConnected to\n",
        "InputLayer\t            (None, 34)\t            0\t\n",
        "Embedding Layer\t        (None, 34, 300)\t        17320500\t        InputLayer\n",
        "Reshape\t                (None, 34, 300, 1)\t    0\t                Embedding Layer\n",
        "Conv2D Block 1\t        (None, 34, 1, 512)\t    154112\t            Reshape\n",
        "MaxPooling2D Block 1\t(None, 1, 1, 512)\t    0\t                Conv2D Block 1\n",
        "Conv2D Block 2\t        (None, 33, 1, 512)\t    307712\t            Reshape\n",
        "MaxPooling2D Block 2\t(None, 1, 1, 512)\t    0\t                Conv2D Block 2\n",
        "Conv2D Block 3\t        (None, 32, 1, 512)\t    461312\t            Reshape\n",
        "MaxPooling2D Block 3\t(None, 1, 1, 512)\t    0\t                Conv2D Block 2\n",
        "Conv2D Block 4\t        (None, 31, 1, 512)\t    614912\t            Reshape\n",
        "MaxPooling2D Block 4\t(None, 1, 1, 512)\t    0\t                Conv2D Block 2\n",
        "Conv2D Block 5\t        (None, 30, 1, 512)\t    768512\t            Reshape\n",
        "MaxPooling2D Block 5\t(None, 1, 1, 512)\t    0\t                Conv2D Block 2\n",
        "Conv2D Block 6\t        (None, 29, 1, 512)\t    922112\t            Reshape\n",
        "MaxPooling2D Block 6\t(None, 1, 1, 512)\t    0\t                Conv2D Block 2\n",
        "Concatenate\t            (None, 6, 1, 512)\t    0\t                All MaxPooling2D Blocks\n",
        "Flatten\t                (None, 3072)\t        0\t                Concatenate\n",
        "Dropout Layer\t        (None, 3072)\t        0\t                Flatten\n",
        "Dense Layer\t            (None, 27)\t            8297\t            Dropout Layer\n",
        "\n",
        "This architecture contains total 20,632,143 trainable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgcE7hzu4noM"
      },
      "source": [
        "* Layer (type)-----------------Output Shape-----------Number of Params------Connected to\n",
        "* InputLaye-------------------(None, 34)--------------0\t\n",
        "* Embedding Layer-----------(None, 34, 300)---------17320500----------------InputLayer\n",
        "* Reshape---------------------(None, 34, 300, 1)------0--------------------------Embedding Layer\n",
        "* Conv2D Block 1-------------(None, 34, 1, 512)------154112-------------------Reshape\n",
        "* MaxPooling2D Block 1------(None, 1, 1, 512)-------0--------------------------Conv2D Block 1\n",
        "* Conv2D Block 2-------------(None, 33, 1, 512)------307712-------------------Reshape\n",
        "* MaxPooling2D Block 2------(None, 1, 1, 512)-------0--------------------------Conv2D Block 2\n",
        "* Conv2D Block 3-------------(None, 32, 1, 512)------461312-------------------Reshape\n",
        "* MaxPooling2D Block 3------(None, 1, 1, 512)-------0--------------------------Conv2D Block 2\n",
        "* Conv2D Block 4-------------(None, 31, 1, 512)------614912-------------------Reshape\n",
        "* MaxPooling2D Block 4------(None, 1, 1, 512)-------0--------------------------Conv2D Block 2\n",
        "* Conv2D Block 5-------------(None, 30, 1, 512)------768512-------------------Reshape\n",
        "* MaxPooling2D Block 5------(None, 1, 1, 512)-------0--------------------------Conv2D Block 2\n",
        "* Conv2D Block 6-------------(None, 29, 1, 512)------922112-------------------Reshape\n",
        "* MaxPooling2D Block 6------(None, 1, 1, 512)-------0--------------------------Conv2D Block 2\n",
        "* Concatenate-----------------(None, 6, 1, 512)-------0--------------------------All MaxPooling2D Blocks\n",
        "* Flatten-----------------------(None, 3072)-----------0--------------------------Concatenate\n",
        "* Dropout Layer---------------(None, 3072)-----------0--------------------------Flatten\n",
        "* Dense Layer-----------------(None, 27)--------------8297----------------------Dropout Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMFqDNZ-4noM"
      },
      "source": [
        "### Benchmark Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWm0I8Xj4noQ"
      },
      "source": [
        "Following are the weighted-F1 score obtained using the benchmark models described above:\n",
        "\n",
        "Text: 0.8113\n",
        "\n",
        "Images: 0.5534\n",
        "\n",
        "As the benchmarking model using text is better performing, the Y benchmark file contains the output of the same."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submitting to the online challenge: [ramp.studio](http://ramp.studio)\n"
      ],
      "metadata": {
        "id": "A34qSj_Ai8TE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}